{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c4572f8",
   "metadata": {},
   "source": [
    "# Supervised Relations Corpus Creation\n",
    "### Merge CCD and CCDt Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce34ecb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ---- Load data ----\n",
    "path_ccd = \"ccd_corpus.json\"\n",
    "with open(path_ccd, encoding='utf-8') as f:\n",
    "    data_ccd = json.load(f)['results']\n",
    "path_ccdt = \"ccdt_corpus.json\"\n",
    "with open(path_ccdt, encoding='utf-8') as f:\n",
    "    data_ccdt = json.load(f)['results']\n",
    "\n",
    "df = pd.read_csv(\"clinical_notes_text.csv\")\n",
    "\n",
    "# ---- Function to apply entity tags using indices ----\n",
    "def add_indexed_tags(text, drug_spans, disease_spans):\n",
    "    spans = []\n",
    "    # Collect spans as (start, end, tag)\n",
    "    for idx, span in enumerate(drug_spans):\n",
    "        spans.append((span['start'], span['end'],\n",
    "                      f\"<drug{idx+1}>{text[span['start']:span['end']]}</drug{idx+1}>\"))\n",
    "    for idx, span in enumerate(disease_spans):\n",
    "        spans.append((span['start'], span['end'],\n",
    "                      f\"<disease{idx+1}>{text[span['start']:span['end']]}</disease{idx+1}>\"))\n",
    "\n",
    "    # 1. Sort spans by length (longer first), then by start index\n",
    "    spans.sort(key=lambda x: (-(x[1] - x[0]), x[0]))\n",
    "\n",
    "    # 2. Remove overlapping spans → keep only the longest non-overlapping ones\n",
    "    non_overlapping = []\n",
    "    for start, end, tagged in spans:\n",
    "        overlap = any(not (end <= s or start >= e) for s, e, _ in non_overlapping)\n",
    "        if not overlap:\n",
    "            non_overlapping.append((start, end, tagged))\n",
    "\n",
    "    # 3. Sort back-to-front to preserve indices during insertion\n",
    "    non_overlapping.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # 4. Insert tags into text\n",
    "    for start, end, tagged in non_overlapping:\n",
    "        text = text[:start] + tagged + text[end:]\n",
    "    return text\n",
    "\n",
    "\n",
    "# ---- Process notes ----\n",
    "df_per_sentences = pd.DataFrame(columns=['id', 'documents'])\n",
    "\n",
    "for note in data_ccd:\n",
    "    hadm_id = note['id']\n",
    "    diseases = note['diseases_predicted']\n",
    "\n",
    "    note_df = df[df['id'] == hadm_id]\n",
    "    if note_df.empty:\n",
    "        continue\n",
    "\n",
    "    # Extract note text\n",
    "    text = note_df.iloc[0]['documents']\n",
    "\n",
    "    # Flatten nested drugs list [[...]] → [...]\n",
    "    drugs_nested = [d['drugs_predicted'] for d in data_ccdt if d['id'] == hadm_id]\n",
    "    drugs = [item for sublist in drugs_nested for item in sublist]\n",
    "\n",
    "    # Clean empty entries\n",
    "    diseases = [d for d in diseases if d]\n",
    "    drugs = [d for d in drugs if d]\n",
    "\n",
    "    if not diseases and not drugs:\n",
    "        continue\n",
    "\n",
    "    # Apply tags\n",
    "    text_tagged = add_indexed_tags(text, drugs, diseases)\n",
    "\n",
    "    # Split by '#' and keep only parts containing both entities\n",
    "    for i, part in enumerate(text_tagged.split('#')):\n",
    "        has_drug = re.search(r\"<drug\\d+>\", part)\n",
    "        has_disease = re.search(r\"<disease\\d+>\", part)\n",
    "        if has_drug and has_disease:\n",
    "            drugs_found = re.findall(r\"<drug\\d+>(.*?)</drug\\d+>\", part)\n",
    "            diseases_found = re.findall(r\"<disease\\d+>(.*?)</disease\\d+>\", part)\n",
    "            \n",
    "            df_per_sentences = pd.concat([df_per_sentences, pd.DataFrame([{\n",
    "                'id': f\"{hadm_id}_{i}\",\n",
    "                'documents': part\n",
    "            }])], ignore_index=True)\n",
    "\n",
    "\n",
    "# ---- Save ----\n",
    "df_per_sentences.to_csv('df_per_sentences_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de68fe1c",
   "metadata": {},
   "source": [
    "### Cluter Predicted Relations to Create Relation Labels List\n",
    "Using the LLaMIC output in flexible mode – meaning the model can generate relation labels freely – we cluster the predicted relations to derive consistent relation categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53473306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.facecolor': '#eeeeee',\n",
    "    'axes.edgecolor': 'gray',\n",
    "    'axes.grid': True,\n",
    "    'grid.color': 'white',\n",
    "    'grid.linestyle': '-',\n",
    "    'grid.alpha': 1.0,\n",
    "    'font.size': 9,\n",
    "    'axes.titlesize': 11,\n",
    "    'axes.labelsize': 9,\n",
    "    'xtick.labelsize': 8,\n",
    "    'ytick.labelsize': 8,\n",
    "    'legend.fontsize': 7,\n",
    "    'legend.title_fontsize': 8,\n",
    "    'figure.dpi': 300\n",
    "})\n",
    "\n",
    "# Load predicted biomedical relations\n",
    "with open(r\"llamic_flexible_results.json\") as f:\n",
    "    data = json.load(f)['results']\n",
    "\n",
    "# Organize extracted relation types with updated terminology\n",
    "categories = {\n",
    "    'CCD-CCD': [],\n",
    "    'CCDt-CCDt': [],\n",
    "    'CCD-CCDt': []\n",
    "}\n",
    "\n",
    "for entry in data:\n",
    "        relations = entry.get('relations', [])\n",
    "        if not relations:\n",
    "            continue\n",
    "\n",
    "        for triplet in relations:\n",
    "            if not isinstance(triplet, list) or len(triplet) != 3:\n",
    "                continue\n",
    "            head, rel, tail = triplet\n",
    "            if '<disease' in head and '<disease' in tail:\n",
    "                categories['CCD-CCD'].append(rel)\n",
    "            elif '<drug' in head and '<drug' in tail:\n",
    "                categories['CCDt-CCDt'].append(rel)\n",
    "            elif ('<disease' in head and '<drug' in tail) or ('<drug' in head and '<disease' in tail):\n",
    "                categories['CCD-CCDt'].append(rel)\n",
    "\n",
    "# Remove duplicates\n",
    "categories = {k: list(set(v)) for k, v in categories.items()}\n",
    "# Load language model for semantic embedding\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Prepare the figure with 3 subplots (vertical)\n",
    "fig, axes = plt.subplots(nrows=3, figsize=(8, 14))\n",
    "fig.subplots_adjust(hspace=0.4, right=0.75)\n",
    "\n",
    "# Color palette\n",
    "colors = plt.get_cmap(\"Set2\").colors\n",
    "\n",
    "# Iterate over categories and axes\n",
    "for ax, (cat, rels) in zip(axes, categories.items()):\n",
    "    if not rels:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "\n",
    "    if len(rels) == 1:\n",
    "        ax.scatter(0, 0, s=80, color=colors[0], label=rels[0])\n",
    "        ax.set_title(f\"Semantic Clustering – {cat}\", weight='bold')\n",
    "        ax.set_xlabel(\"PC1\")\n",
    "        ax.set_ylabel(\"PC2\")\n",
    "        ax.spines[['top', 'right']].set_visible(False)\n",
    "        ax.legend(\n",
    "            loc='upper left',\n",
    "            bbox_to_anchor=(1.02, 1),\n",
    "            borderaxespad=0.5,\n",
    "            title=\"Representative Relation\",\n",
    "            frameon=True\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    embeddings = model.encode(rels)\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(embeddings)\n",
    "\n",
    "    k = min(10, len(rels))\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    clusters = kmeans.fit_predict(reduced)\n",
    "\n",
    "    cluster_labels = {}\n",
    "    for idx, label in enumerate(rels):\n",
    "        cluster_id = clusters[idx]\n",
    "        if cluster_id not in cluster_labels:\n",
    "            cluster_labels[cluster_id] = label\n",
    "\n",
    "    for cluster_id in range(k):\n",
    "        points = reduced[np.array(clusters) == cluster_id]\n",
    "        ax.scatter(points[:, 0], points[:, 1], s=50, alpha=0.85,\n",
    "                   color=colors[cluster_id % len(colors)],\n",
    "                   label=cluster_labels[cluster_id])\n",
    "\n",
    "    ax.set_title(f\"Semantic Clustering – {cat}\", weight='bold')\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    ax.legend(\n",
    "        loc='upper left',\n",
    "        bbox_to_anchor=(1.02, 1),\n",
    "        borderaxespad=0.5,\n",
    "        title=\"Representative Relation\",\n",
    "        frameon=True\n",
    "    )\n",
    "\n",
    "\n",
    "# Save and show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb9ced",
   "metadata": {},
   "source": [
    "## Input for BioLinkBert\n",
    "Converts CSVs with annotated clinical sentences into JSON for BioLinkBERT, marking entity pairs as [E1]/[E2] and assigning relation labels The BioLinkBERT model is available at: https://github.com/michiyasunaga/LinkBERT/tree/main/src/seqcls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1bb213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load CSVs\n",
    "train_path = \"../data/train.csv\" # supervised corpus\n",
    "dev_path = \"../data/dev.csv\" # supervised corpus\n",
    "test_path = \"../data/test.csv\" # supervised corpus\n",
    "\n",
    "train_table = pd.read_csv(train_path)\n",
    "dev_table = pd.read_csv(dev_path)\n",
    "test_table = pd.read_csv(test_path)\n",
    "\n",
    "# Convert relations string to list of dicts\n",
    "for df in [train_table, dev_table, test_table]:\n",
    "    df['relations'] = df['relations'].apply(lambda x: eval(x) if pd.notna(x) else [])\n",
    "\n",
    "# Function to extract the entity ID\n",
    "def extract_entity_id(text):\n",
    "    match = re.search(r'<(disease|drug)(\\d+)>', text)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}{match.group(2)}\"\n",
    "    return None\n",
    "\n",
    "# Mark entities as [E1] and [E2]\n",
    "def annotate_entities(text, e1_id, e2_id):\n",
    "    def repl(match):\n",
    "        tag, idx, content = match.groups()\n",
    "        full_id = f\"{tag.lower()}{idx}\"\n",
    "        if full_id == e1_id:\n",
    "            return f\"[E1] {content} [/E1]\"\n",
    "        elif full_id == e2_id:\n",
    "            return f\"[E2] {content} [/E2]\"\n",
    "        return content\n",
    "    return re.sub(r'<(disease|drug)(\\d+)>(.*?)</\\1\\2>', repl, text)\n",
    "\n",
    "# Find all entities in the sentence\n",
    "def find_all_entities(text):\n",
    "    return re.findall(r'<(disease|drug)(\\d+)>', text)\n",
    "\n",
    "# Main function to process the annotations\n",
    "def generate_annotated_pairs(df):\n",
    "    output_dataset = []\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row['documents']\n",
    "        triplets = row['relations']\n",
    "\n",
    "        entities = find_all_entities(sentence)\n",
    "        entity_ids = [f\"{t[0]}{t[1]}\" for t in entities]\n",
    "\n",
    "        # Generate unique combinations of entity pairs\n",
    "        for e1_id, e2_id in combinations(entity_ids, 2):\n",
    "            annotated_text = annotate_entities(sentence, e1_id, e2_id)\n",
    "            \n",
    "            # Check if there is a relation in any order\n",
    "            label = \"NA\"\n",
    "            for t in triplets:\n",
    "                h = extract_entity_id(t[0])\n",
    "                t_ = extract_entity_id(t[2])\n",
    "                if {h, t_} == {e1_id, e2_id}:  # relation regardless of order\n",
    "                    label = t[1]\n",
    "                    break\n",
    "\n",
    "            output_dataset.append({\n",
    "                \"text\": annotated_text,\n",
    "                \"label\": label\n",
    "            })\n",
    "    return output_dataset\n",
    "\n",
    "# Generate datasets\n",
    "train_data = generate_annotated_pairs(train_table)\n",
    "val_data = generate_annotated_pairs(dev_table)\n",
    "test_data = generate_annotated_pairs(test_table)\n",
    "\n",
    "# Create output directories inside the input file directories\n",
    "train_output_dir = os.path.join(os.path.dirname(train_path), \"input_biolinkbert\")\n",
    "dev_output_dir = os.path.join(os.path.dirname(dev_path), \"input_biolinkbert\")\n",
    "test_output_dir = os.path.join(os.path.dirname(test_path), \"input_biolinkbert\")\n",
    "\n",
    "for d in [train_output_dir, dev_output_dir, test_output_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "# Save JSON files\n",
    "with open(os.path.join(train_output_dir, \"train.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(dev_output_dir, \"validation.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(test_output_dir, \"test.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\" Data saved:\\n  Train: {len(train_data)}\\n  Validation: {len(val_data)}\\n  Test: {len(test_data)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
