{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3c2b13f",
   "metadata": {},
   "source": [
    "# Supervised Corpus Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a7bf5d",
   "metadata": {},
   "source": [
    "\n",
    "### Process BENT Output\n",
    "After using BENT to predict entities, the predicted entities were masked in the clinical notes in order to allow LLaMIC to extract entities a second time from the modified notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538368a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def blank_all_predicted_entities(document, entities, context_chars=20):\n",
    "    \"\"\"\n",
    "    Replace all occurrences of predicted entities in the document with underscores,\n",
    "    preserving the original length of the entity.\n",
    "    Prints each entity in the document surrounded by < > with some context before blanking.\n",
    "    \"\"\"\n",
    "    for entity in entities:\n",
    "        pattern = r'\\b' + re.escape(entity) + r'\\b'\n",
    "        matches = list(re.finditer(pattern, document, flags=re.IGNORECASE))\n",
    "        for match in matches:\n",
    "            start, end = match.start(), match.end()\n",
    "            # Determine context boundaries\n",
    "            context_start = max(0, start - context_chars)\n",
    "            context_end = min(len(document), end + context_chars)\n",
    "            snippet = document[context_start:start] + \"<\" + document[start:end] + \">\" + document[end:context_end]\n",
    "            \n",
    "            #print(f\"Context preview: ...{snippet}...\")\n",
    "            \n",
    "            # Replace entity with underscores\n",
    "            blanked = '_' * (end - start)\n",
    "            document = document[:start] + blanked + document[end:]\n",
    "    return document\n",
    "\n",
    "def extract_entities(data, entity_type='disease'):\n",
    "    \"\"\"\n",
    "    Extract unique predicted entities of the specified type from the data.\n",
    "    \"\"\"\n",
    "    key_map = {\n",
    "        'disease': 'diseases_predicted',\n",
    "        'drug': 'drugs_predicted'\n",
    "    }\n",
    "    entities = []\n",
    "    for item in data:\n",
    "        for e in item.get(key_map[entity_type], []):\n",
    "            entities.append(e['entity'])\n",
    "    return sorted(set(entities))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load JSON data\n",
    "    input_file = \"bent_predicts.json\"  # BENT predicts JSON file\n",
    "    input_document = \"clinical_notes.csv.gz\"  # Clinical notes CSV file\n",
    "    output_document = \"clinical_notes_blanked.csv.gz\"\n",
    "    with open(input_file) as f:\n",
    "        data = json.load(f)['results']\n",
    "\n",
    "    # Load CSV data\n",
    "    df = pd.read_csv(input_document, compression='gzip')\n",
    "\n",
    "    # Keep only the rows where id is in the JSON\n",
    "    df_filtered = df[df['id'].isin([note['id'] for note in data])].copy()\n",
    "\n",
    "    for note in data:\n",
    "        note_id = note['id']\n",
    "        entities = extract_entities([note], entity_type='disease')\n",
    "\n",
    "        documents = df_filtered.loc[df_filtered['id'] == note_id, 'documents'].values[0]\n",
    "\n",
    "        # Replace predicted entities with blanks, printing them before\n",
    "        blanked_document = blank_all_predicted_entities(documents, entities)\n",
    "\n",
    "        # Update the documents in the filtered dataframe\n",
    "        df_filtered.loc[df_filtered['id'] == note_id, 'documents'] = blanked_document\n",
    "\n",
    "    # Save only the filtered, blanked notes\n",
    "    df_filtered.to_csv(output_document, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240ac44",
   "metadata": {},
   "source": [
    "### Merge BENT and LLaMIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2c44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def merge_entity_jsons(json_file1, json_file2, output_file):\n",
    "    \"\"\"\n",
    "    Merge two JSON files containing entity predictions without duplicating entities.\n",
    "    Duplicates are defined by ('entity', 'icd'/'mesh') for each entity type.\n",
    "    \"\"\"\n",
    "    # Load first JSON\n",
    "    with open(json_file1) as f1:\n",
    "        data1 = json.load(f1)['results']\n",
    "\n",
    "    # Load second JSON\n",
    "    with open(json_file2) as f2:\n",
    "        data2 = json.load(f2)['results']\n",
    "\n",
    "    # Merge entities by note id\n",
    "    merged_dict = {}\n",
    "\n",
    "    for item in data1 + data2:\n",
    "        note_id = item['id']\n",
    "        if note_id not in merged_dict:\n",
    "            merged_dict[note_id] = item.copy()\n",
    "        else:\n",
    "            for key, id_field in [('diseases_predicted', 'icd'), ('drugs_predicted', 'mesh')]:\n",
    "                if key in item:\n",
    "                    existing_entities = merged_dict[note_id].get(key, [])\n",
    "                    new_entities = item[key]\n",
    "\n",
    "                    # Avoid duplicates based on entity text + id_field\n",
    "                    existing_set = {(e['entity'], e.get(id_field)) for e in existing_entities}\n",
    "                    for e in new_entities:\n",
    "                        if (e['entity'], e.get(id_field)) not in existing_set:\n",
    "                            print(f\"Adding new entity to note {note_id}: {e['entity']} ({id_field})\")\n",
    "                            existing_entities.append(e)\n",
    "                    merged_dict[note_id][key] = existing_entities\n",
    "\n",
    "    # Convert merged dictionary back to list\n",
    "    merged_data = list(merged_dict.values())\n",
    "\n",
    "    # Save merged JSON\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({'results': merged_data}, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Merged JSON saved to: {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bent_file = \"bent_predicts.json\"\n",
    "    llamic_file = \"llamic_predicts_on_blanked_notes.json\"\n",
    "    output_file = \"entities_merged.json\"\n",
    "    merge_entity_jsons(bent_file, llamic_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e852fed",
   "metadata": {},
   "source": [
    "### Cluter Predicted Entities to Manual Correct\n",
    "Cluster predicted entities for subsequent manual curation to remove incorrect entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "def cluster_entities(entities, eps=0.5, min_samples=2):\n",
    "    # TF-IDF vectorization\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(entities)\n",
    "\n",
    "    # Compute cosine similarity and convert to distance\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "    distance_matrix = np.clip(1 - cosine_sim, 0, 1)\n",
    "\n",
    "    # DBSCAN clustering\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "    labels = dbscan.fit_predict(distance_matrix)\n",
    "\n",
    "    # Group entities by cluster\n",
    "    clusters = {}\n",
    "    for label, entity in zip(labels, entities):\n",
    "        clusters.setdefault(label, []).append(entity)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def save_clusters_to_file(clusters, filename):\n",
    "    \"\"\"Save clusters dictionary to a text file.\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        for label, entity_list in clusters.items():\n",
    "            f.write(f\"Cluster {label}:\\n\")\n",
    "            for entity in entity_list:\n",
    "                f.write(f\" - {entity}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'automatic_predictions.json'\n",
    "    with open(input_file) as f:\n",
    "        data = json.load(f)['results']\n",
    "\n",
    "    # Extract entities\n",
    "    entities = extract_entities(data, entity_type=entity_type)\n",
    "    # Cluster entities\n",
    "    clusters = cluster_entities(entities, eps=0.5, min_samples=2)\n",
    "    save_clusters_to_file(clusters, f\"clusters_{entity_type}s.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb073c6",
   "metadata": {},
   "source": [
    "Script to update the JSON file by removing entities that were excluded during cluster curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a24670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def load_kept_entities(cluster_file):\n",
    "    \"\"\"\n",
    "    Read manually edited cluster file and return a set of entities to keep.\n",
    "    \"\"\"\n",
    "    kept_entities = set()\n",
    "    with open(cluster_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('- '):\n",
    "                entity = line[2:].strip()\n",
    "                kept_entities.add(entity)\n",
    "    print(f\"Kept entities loaded: {kept_entities}\")\n",
    "    return kept_entities\n",
    "\n",
    "def filter_entities_in_json(input_json_file, output_json_file, kept_entities):\n",
    "    \"\"\"\n",
    "    Filter the JSON so that only entities present in kept_entities are preserved.\n",
    "    \"\"\"\n",
    "    with open(input_json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for note in data['results']:\n",
    "        # Filter diseases\n",
    "        if 'diseases_predicted' in note:\n",
    "            filtered_diseases = []\n",
    "            for e in note.get('diseases_predicted', []):\n",
    "                if e['entity'] in kept_entities:\n",
    "                    filtered_diseases.append(e)\n",
    "                else:\n",
    "                    print(f\"Removed entity: '{e['entity']}' from note id {note['id']}\")\n",
    "            note['diseases_predicted'] = filtered_diseases\n",
    "\n",
    "        # Filter drugs (optional, uncomment if needed)\n",
    "        if 'drugs_predicted' in note:\n",
    "            filtered_drugs = []\n",
    "            for e in note.get('drugs_predicted', []):\n",
    "                if e['entity'] in kept_entities:\n",
    "                    filtered_drugs.append(e)\n",
    "                else:\n",
    "                    print(f\"Removed entity: '{e['entity']}' from note id {note['id']}\")\n",
    "            note['drugs_predicted'] = filtered_drugs\n",
    "\n",
    "    # Save updated JSON\n",
    "    with open(output_json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Filtered JSON saved to: {output_json_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_json_file = 'automatic_predictions.json'\n",
    "    output_json_file = 'automatic_predictions_filtered.json'\n",
    "    cluster_file = 'clusters_diseases.txt' # manually edited file\n",
    "\n",
    "    kept_entities = load_kept_entities(cluster_file)\n",
    "    filter_entities_in_json(input_json_file, output_json_file, kept_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae4674",
   "metadata": {},
   "source": [
    "### Correct Terminologies\n",
    "Create a set of entities with their corresponding ICD or MeSH codes in a TXT file, one pair per line, to allow subsequent manual curation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6dcc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_entities_to_txt(json_file, output_txt_file):\n",
    "    \"\"\"\n",
    "    Extract entities and their ICD/MeSH codes from the JSON and save them to a TXT file.\n",
    "    Each line: entity \\t id\n",
    "    \"\"\"\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)['results']\n",
    "\n",
    "    lines = set()  # Use set to avoid duplicates\n",
    "\n",
    "    for note in data:\n",
    "        for key, id_field in [('diseases_predicted', 'icd'), ('drugs_predicted', 'mesh')]:\n",
    "            for e in note.get(key, []):\n",
    "                entity_text = e.get('entity', '').strip()\n",
    "                entity_id = e.get(id_field, '').strip()\n",
    "                if entity_text and entity_id:\n",
    "                    lines.add(f\"{entity_text}\\t{entity_id}\")\n",
    "\n",
    "    # Save to TXT file, one entity per line\n",
    "    with open(output_txt_file, 'w', encoding='utf-8') as f:\n",
    "        for line in sorted(lines):\n",
    "            f.write(line + '\\n')\n",
    "\n",
    "    print(f\"Entities saved to {output_txt_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_json_file = \"automatic_predictions_filtered.json\"\n",
    "    output_txt_file = \"entities_for_manual_correction.txt\"\n",
    "    save_entities_to_txt(input_json_file, output_txt_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed9e493",
   "metadata": {},
   "source": [
    "Script to update the JSON file by updating ICD or MeSH codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633fe9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_kept_entities(txt_file):\n",
    "    \"\"\"\n",
    "    Load manually corrected entities from a TXT file.\n",
    "    Each line should have: entity \\t id\n",
    "    Returns a dictionary: {entity: id}\n",
    "    \"\"\"\n",
    "    kept_entities = {}\n",
    "    with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                entity, entity_id = parts\n",
    "                kept_entities[entity.strip()] = entity_id.strip()\n",
    "    return kept_entities\n",
    "\n",
    "def update_json_with_kept_entities(input_json, txt_file, output_json):\n",
    "    \"\"\"\n",
    "    Update the JSON, keeping only entities present in the manually corrected TXT.\n",
    "    The ID (icd/mesh) is updated according to the TXT.\n",
    "    \"\"\"\n",
    "    kept_entities = load_kept_entities(txt_file)\n",
    "\n",
    "    with open(input_json, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for note in data['results']:\n",
    "        for key, id_field in [('diseases_predicted', 'icd'), ('drugs_predicted', 'mesh')]:\n",
    "            filtered_entities = []\n",
    "            for e in note.get(key, []):\n",
    "                entity_name = e['entity']\n",
    "                if entity_name in kept_entities:\n",
    "                    # Update the id (icd or mesh) according to the TXT\n",
    "                    e[id_field] = kept_entities[entity_name]\n",
    "                    filtered_entities.append(e)\n",
    "                else:\n",
    "                    print(f\"Removed entity '{entity_name}' from note {note['id']}\")\n",
    "            note[key] = filtered_entities\n",
    "\n",
    "    # Save updated JSON\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Updated JSON saved to: {output_json}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_json = \"automatic_predictions_filtered.json\"\n",
    "    txt_file = \"entities_for_manual_correction.txt\"\n",
    "    output_json = \"semi-automatic_predictions.json\"\n",
    "    update_json_with_kept_entities(input_json, txt_file, output_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421684a0",
   "metadata": {},
   "source": [
    "After this manual pre-correction, run the script `note_level_entity_correction.py` on `semi_automatic_predictions.json` to perform corrections at the note and entity levels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
